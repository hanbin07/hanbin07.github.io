<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Research</title>
    <link rel ="stylesheet" href="css/bootstrap.css">
    <script src="//code.jquery.com/jquery-1.11.0.min.js"></script>
    <script type="text/javascript">
        $(document).ready( function() {
            $("#sidebar-wrapper").load("SideBar.html");  // 원하는 파일 경로를 삽입하면 된다
            $("#footers").load("footer.html");  // 추가 인클루드를 원할 경우 이런식으로 추가하면 된다
        });

        function Show() {
            document.getElementById("pdfview").style.display = "block";
            document.getElementById("show").style.display = "none";
            document.getElementById("hide").style.display = "block";
        }
        function hide() {
            document.getElementById("pdfview").style.display = "none";
            document.getElementById("show").style.display = "block";
            document.getElementById("hide").style.display = "none";
        }
    </script>
</head>

<body>
<div id="page-wrapper">
    <!-- 사이드바 -->
    <div id="sidebar-wrapper"></div>

    <!-- 본문 -->
    <div id="page-content-wrapper">
        <div class="container-fluid">
            
            <h1>Research</h1>
            
            <br> <br>

            <h4>Real-Time Facial Animation Generation on Face Mask<br></h4>

            <div class="container-fluid">

                <h4> <i>SIGGRAPH ASIA 2022 Poster</i></h4>
                <div class="container-fluid"> 
                    <div style="font-size: 20px;"> Abstract</div>

                    In light of the COVID-19 pandemic, wearing a mask is crucial to avoid contracting infectious diseases. 
                    However, wearing a mask is known to impair communication functions. 
                    This study aims to address the communication difficulties caused by wearing a mask and provide a strategy for aiding in understanding the speaker’s speech through facial animation. 
                    Facial animation is generated in real-time, and upper facial information is processed to detect the speaker’s emotions, generating a lower facial expression. 
                    In addition, the system detects the mask’s shape and enables accurate registration in the proper position. 
                    This technology can improve communication and alleviate challenges associated with communication between persons wearing face masks.
                    </br>
                    <a href="https://dl.acm.org/doi/abs/10.1145/3550082.3564178">[Paper link]</a><br> <br>
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/0lRZAbqq9hc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </iframe>
                </div>
                <br>
            </div>
            

            <h4>Mixing in Reverse Optical Flow to Mitigate Vection and Simulation Sickness in Virtual Reality<br></h4>

            <div class="container-fluid">

                <h4> <i>CHI 2022 conference paper</i></h4>
                <div class="container-fluid"> 
                    <div style="font-size: 20px;"> Abstract</div>

                    In this study, we explore a method to reduce simulator sickness by visually mixing the optical flow patterns that are in the reverse direction of the virtual visual motion. 
                    As visual motion is mainly detected and perceived by the optical flow, artificial mixing in the reverse flow is hypothesized to induce a cancellation effect, thereby reducing the 
                    degree of the conflict with the vestibular sense and sickness. To validate our hypothesis, we developed a real-time algorithm to visualize the reverse optical flow and conducted experiments 
                    by comparing the before and after sickness levels in seven virtual navigation conditions.  
                    </br>
                    <a href="https://dl.acm.org/doi/10.1145/3491102.3501847">[Paper link]</a><br> <br>
                <iframe width="560" height="315" src="https://www.youtube.com/embed/LnAMuqbwQyo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
                </iframe>
                </div>
                <br>
            </div>
            
            <h4>AudienceMR: Extending the Local Space for Large-Scale Audience with Mixed Reality for Enhanced Remote Lecturer Experience<br></h4>
            
            <div class="container-fluid">
                
                <h4> <i> Applied science </i></h4>
                
                <div class="container-fluid"> 
                    <div style="font-size: 20px;"> Abstract</div>
                    AudienceMR is designed as a multi-user mixed reality space that seamlessly extends the local user space to become a large, 
                    shared classroom where some of the audience members are seen seated in a real space, and more members are seen through an extended portal. <br>
                    AudienceMR can provide a sense of the presence of a large-scale crowd/audience with the associated spatial context. In contrast to virtual reality (VR), 
                    however, with mixed reality (MR), a lecturer can deliver content or conduct a performance from a real, actual, comfortable, and familiar local space, while <br>
                    interacting directly with real nearby objects, such as a desk, podium, educational props, instruments, and office materials.<br>
                
                    <a href="https://www.mdpi.com/2076-3417/11/19/9022">[Paper link]</a><br> <br>
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/idSctVJI4vo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </div>
                <br>
            
            </div>
            
            <h4>Application of Semantic Web technologies in Informatics Education<br></h4>
            
            <div class="container-fluid">
                
                <h4> <i>ISSEP 2020</i></h4>
                
                <div class="container-fluid"> 
                    <div style="font-size: 20px;"> Abstract</div>
                    In this paper, we propose to use a Semantic Web reasoner in order to help students understand concepts in informatics. 
                    More specifically, our approach consists of three parts.<br> First, learning materials are collected from DBpedia (https://wiki.dbpedia.org) that is an online 
                    knowledge base whose contents are provided in several formats including RDF, Notation3, etc. <br> Second, a teacher defines inference rules according to which 
                    new facts can be derived from the set of learning materials taken from DBpedia.<br> Third, a learner executes a system called EBRS that is a simple-to-use interface 
                    for a well-known Semantic Web reasoner, EYE (http://eulersharp.sourceforge.net/). EBRS shows the result of reasoning on a screen so that learners can understand 
                    the relationship of learning materials easily.<br> The theme of this research is to exploit Semantic Web technologies such as RDF, ontologies, reasoning programs, etc
                     in order to support informatics education.
                     <br>
                    <a href="http://ceur-ws.org/Vol-2755/shortpaper2.pdf">[Paper link]</a><br> <br>
                </div>
                <br>

            </div>

            <h4>2D/3D Mixed Interface for Furniture Placement in Smartphone-based Mobile Augmented Reality</h4>
            
            <div class="container-fluid">
            
               <h4> <i>VRST 2019</i></h4>
               
               <div class="container-fluid"> 
                
                <div style="font-size: 20px;"> Abstract</div>
                    We propose to use an approximate 2D map of the environment generated from the latest environment modeling technology 
                    and enhance the object manipulation performance for the touch based mobile augmented reality.
                    We validated the advantage of the proposed interface through a pilot experiment 
                    and confirmed that the use of the 2D map helps reduce the task completion time almost 2 times and improve the usability as well.<br> <br>
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/MqK0znjkY-Q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                
                    <br>
                <a href="https://dl.acm.org/doi/10.1145/3359996.3364715">[Paper link]</a><br> <br>
                </div>
                <br> 
            </div>

            <h4>A P-NP predicate with one variable whose domain is a set of worlds<br></h4>

            <div class="container-fluid">
                
                <h4> <i>Godel’s LEGACY : DOES FUTURE LIE IN THE PAST?</i></h4>
                
                A P-NP predicate with one variable whose domain is a set of worlds<br>
                
                <img src="images/goedel_medal.jpg" width="20%"><br>
                
                <a href="https://drive.google.com/open?id=1mR-abSScq0m1FVMSNQwcMxu6By2xk_l7">Poster link</a><br> <br>
            </div>


        </div>
    </div>
    <!-- /본문 -->
    <div id="footers"></div>
</div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
<script type="text/javascript" src="js/bootstrap.js"></script>
</body>

</html>
